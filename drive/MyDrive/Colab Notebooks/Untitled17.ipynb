{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSp0eElXVer8XueUqyKE79"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwOgWrW9tyuh","outputId":"cd3ef360-1703-40e1-8289-4be7e323b081"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["üì• Downloading latest Consumer Complaint dataset...\n"]}],"source":["# ==========================================================\n","# PROJECT: Consumer Complaint Text Classification\n","# COMPANY: Kaiburr\n","# AUTHOR: Sanathraj S\n","# ==========================================================\n","\n","# -------------------------------\n","# Step 0: Import Libraries\n","# -------------------------------\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import string\n","import nltk\n","import requests\n","import io\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('stopwords')\n","\n","# -------------------------------\n","# Step 1: Load Dataset (Automatic)\n","# -------------------------------\n","# Fetch data directly from CFPB API (official source)\n","print(\"üì• Downloading latest Consumer Complaint dataset...\")\n","url = \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\"\n","response = requests.get(url)\n","\n","if response.status_code == 200:\n","    df = pd.read_csv(io.BytesIO(response.content), compression='zip', low_memory=False)\n","    print(\"‚úÖ Dataset successfully downloaded!\")\n","else:\n","    print(\"‚ùå Download failed, please check internet connection.\")\n","\n","print(\"Dataset shape:\", df.shape)\n","print(\"Columns:\", df.columns.tolist()[:10])\n","\n","# -------------------------------\n","# Step 2: Select Relevant Columns\n","# -------------------------------\n","df = df[['Product', 'Consumer complaint narrative']]\n","df.dropna(inplace=True)\n","\n","# -------------------------------\n","# Step 3: Filter Required Categories\n","# -------------------------------\n","category_map = {\n","    'Credit reporting, repair, or other personal consumer reports': 0,\n","    'Debt collection': 1,\n","    'Consumer Loan': 2,\n","    'Mortgage': 3\n","}\n","\n","df = df[df['Product'].isin(category_map.keys())]\n","df['Category'] = df['Product'].map(category_map)\n","df.reset_index(drop=True, inplace=True)\n","\n","print(\"\\nCategory Distribution:\")\n","print(df['Product'].value_counts())\n","\n","# -------------------------------\n","# Step 4: Exploratory Data Analysis\n","# -------------------------------\n","plt.figure(figsize=(8,4))\n","sns.countplot(y='Product', data=df, order=df['Product'].value_counts().index, palette='viridis')\n","plt.title(\"Complaint Count by Category\")\n","plt.xlabel(\"Count\")\n","plt.ylabel(\"Category\")\n","plt.show()\n","\n","# Wordcloud\n","text = \" \".join(df['Consumer complaint narrative'].astype(str).tolist())\n","wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n","plt.figure(figsize=(10,5))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.title(\"Most Frequent Words in Complaints\")\n","plt.show()\n","\n","# -------------------------------\n","# Step 5: Text Cleaning Function\n","# -------------------------------\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)  # Keep only letters and spaces\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in text.split() if word not in stop_words]\n","    return \" \".join(tokens)\n","\n","df['clean_text'] = df['Consumer complaint narrative'].apply(clean_text)\n","\n","# -------------------------------\n","# Step 6: Train-Test Split\n","# -------------------------------\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['clean_text'], df['Category'], test_size=0.2, random_state=42, stratify=df['Category']\n",")\n","\n","# -------------------------------\n","# Step 7: Model Selection and Training\n","# -------------------------------\n","models = {\n","    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n","    \"Naive Bayes\": MultinomialNB(),\n","    \"Linear SVM\": LinearSVC(),\n","    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42)\n","}\n","\n","results = {}\n","\n","for name, model in models.items():\n","    print(f\"\\nüîπ Training {name}...\")\n","    pipeline = Pipeline([\n","        ('tfidf', TfidfVectorizer(max_features=7000, ngram_range=(1,2))),\n","        ('clf', model)\n","    ])\n","    pipeline.fit(X_train, y_train)\n","    y_pred = pipeline.predict(X_test)\n","    acc = accuracy_score(y_test, y_pred)\n","    results[name] = acc\n","    print(f\"{name} Accuracy: {acc:.4f}\")\n","    print(classification_report(y_test, y_pred))\n","\n","# -------------------------------\n","# Step 8: Model Comparison\n","# -------------------------------\n","plt.figure(figsize=(7,4))\n","sns.barplot(x=list(results.keys()), y=list(results.values()), palette='mako')\n","plt.title(\"Model Accuracy Comparison\")\n","plt.ylabel(\"Accuracy\")\n","plt.ylim(0, 1)\n","plt.show()\n","\n","best_model_name = max(results, key=results.get)\n","print(f\"\\nüèÜ Best Model: {best_model_name} ({results[best_model_name]:.2%})\")\n","\n","# -------------------------------\n","# Step 9: Evaluate Best Model\n","# -------------------------------\n","final_pipeline = Pipeline([\n","    ('tfidf', TfidfVectorizer(max_features=7000, ngram_range=(1,2))),\n","    ('clf', models[best_model_name])\n","])\n","\n","final_pipeline.fit(X_train, y_train)\n","y_pred_final = final_pipeline.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred_final)\n","plt.figure(figsize=(6,5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',\n","            xticklabels=list(category_map.keys()),\n","            yticklabels=list(category_map.keys()))\n","plt.title(f\"{best_model_name} Confusion Matrix\")\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","plt.xticks(rotation=75)\n","plt.show()\n","\n","# -------------------------------\n","# Step 10: Prediction on New Complaints\n","# -------------------------------\n","reverse_map = {v:k for k,v in category_map.items()}\n","sample_complaints = [\n","    \"They keep calling me for a debt I already paid last year.\",\n","    \"There are incorrect accounts showing on my credit report.\",\n","    \"My mortgage application is delayed for no reason.\",\n","    \"The consumer loan interest rate they charged is wrong.\"\n","]\n","\n","preds = final_pipeline.predict(sample_complaints)\n","\n","print(\"\\nüìä Sample Predictions:\")\n","for text, label in zip(sample_complaints, preds):\n","    print(f\"Complaint: {text}\\nPredicted Category: {reverse_map[label]}\\n\")\n","\n","print(\"\\nüéØ Final Model Ready for Deployment and Submission\")\n"]}]}